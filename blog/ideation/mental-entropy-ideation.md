---
type: 'blog'
path: "/blog/hmm"
title: "Mental Entropy"
subtitle: ""
summary: ""
date: "2018-03-30"
published: false
---
Problem: making plans in one state of mind and failing to follow through with later states of mine.

Mental Entropy: If there are more states of mind in which desire for an action will not arise, then the action will tend not to be taken. The way to "fight" entropy is to introduce energy into the system being measured. Energy

If you experience more states of mind in which there is a lack of desire to take a specific action, then you shouldn't expect to be successful.

Outline:
- The main point
- The principle
- Introduce statistical entropy
- Explain mental entropy

V1.
When a goal requires the performance of future actions, if a person experiences more states of mind in which there is a lack of desire to take those actions, the probability of success will be low.

V2.
When setting a goal that requires the regular performance of an action, if you experience more states of mind in which there is a lack of desire to take the specific action, you should not expect to successfully reach your goal.

V3.
When the required actions of a goal depend on a mind that produces more states that lack a desire to perform the actions, the probability of success is low.

V4.
The probability of successfully reaching a goal is determined by the ratio of states of mind that desire to take the required actions and those that do not.

--Begin--

Why do we fail to reach many of our goals in life? There are myriad books that set out to solve this problem, but I haven't yet come across one that reaches the fundamental problem (as I see it). That problem is what I think of as mental entropy:

> The probability of performing a future action is determined by the relationship between the number of possible mental states and the number of mental states with an active desire to perform the action.  

Or, put more simply:
When your current state of mind resolves to take a future action, but you tend to produce more states of mind free of such resolve, you should expect that your future self **will not** take the action no matter how much you want to believe it will.


Most people probably consider entropy to be a measure of disorder—something with high entropy means highly random while low entropy means highly ordered. But the accuracy of the definition depends greatly on what is being referred to by the word "order."


 With statistical entropy, we can explain that a puff of smoke disperses in a room because there are more arrangements of the smoke particles where they are spread out than where they are bunched together. The common—though slightly incorrect—example of entropy is in considering how a closet tends to be more disorganized than organized because there are more configurations of "messy" than organized. This last example is slightly incorrect because it ignores the reason for why order is important: it determines the ability of a system to do work. In thermodynamics, for example, the flow of energy from a usable (ordered) state to an unusable (unordered) state is defined by entropy. In information theory, entropy is is a measure of information conveyed. 
